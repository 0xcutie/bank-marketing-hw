{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最终处理后的数据：\n",
      "        age  education  housing  loan  contact  month  day_of_week  campaign  \\\n",
      "0 -0.980752          9        1     0        1      5            5 -0.209228   \n",
      "1 -0.107991         11        0     0        0      5            5  0.569634   \n",
      "2 -1.465619         11        1     0        0      6            3 -0.598660   \n",
      "3 -0.204965          9        1     0        0      6            5  0.180203   \n",
      "4  0.667795         14        1     0        1     11            1 -0.598660   \n",
      "\n",
      "   previous  poutcome  ...  job_management  job_retired  job_self-employed  \\\n",
      "0 -0.351356         0  ...               0            0                  0   \n",
      "1 -0.351356         0  ...               0            0                  0   \n",
      "2 -0.351356         0  ...               0            0                  0   \n",
      "3 -0.351356         0  ...               0            0                  0   \n",
      "4 -0.351356         0  ...               0            0                  0   \n",
      "\n",
      "   job_services  job_student  job_technician  job_unemployed  \\\n",
      "0             0            0               0               0   \n",
      "1             1            0               0               0   \n",
      "2             1            0               0               0   \n",
      "3             1            0               0               0   \n",
      "4             0            0               0               0   \n",
      "\n",
      "   marital_divorced  marital_married  marital_single  \n",
      "0                 0                1               0  \n",
      "1                 0                0               1  \n",
      "2                 0                1               0  \n",
      "3                 0                1               0  \n",
      "4                 0                1               0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer  # 处理缺失值\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. 读取数据\n",
    "file_path = \"/Users/cleazhang/Downloads/bank-additional.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep=',', engine='python')  # 以 `,` 为分隔符\n",
    "df.columns = df.columns.str.strip()  # 去除列名中的空格\n",
    "\n",
    "# === 处理数据的完整副本 ===\n",
    "df_processed = df.copy()\n",
    "\n",
    "# === 1.删除不相关列 ===\n",
    "to_delete_features = ['default','pdays']\n",
    "df_processed.drop(columns=to_delete_features, inplace=True, errors='ignore')  # errors='ignore' 防止列不存在时报错\n",
    "\n",
    "# === 2. 处理分类变量缺失值 ===\n",
    "# 众数填充（Mode Impute）适用于分类变量\n",
    "mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "categorical_features = [\"job\", \"marital\", \"education\", \"housing\", \"loan\", \"contact\", \"poutcome\"]\n",
    "df_processed[categorical_features] = df_processed[categorical_features].replace(\"unknown\", np.nan)\n",
    "df_processed[categorical_features] = mode_imputer.fit_transform(df_processed[categorical_features])\n",
    "\n",
    "# === 3. Label Encoding ===\n",
    "label_encoding_dict = {\n",
    "    \"education\": {'illiterate': 0, 'basic.4y': 4, 'basic.6y': 6, 'basic.9y': 9,\n",
    "                  'high.school': 11, 'professional.course': 13, 'university.degree': 14},\n",
    "    \"housing\": {'no': 0, 'yes': 1},\n",
    "    \"loan\": {'no': 0, 'yes': 1},\n",
    "    \"contact\": {'telephone': 0, 'cellular': 1},\n",
    "    \"month\": {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "              'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12},\n",
    "    \"day_of_week\": {'mon': 1, 'tue': 2, 'wed': 3, 'thu': 4, 'fri': 5},\n",
    "    \"poutcome\": {'nonexistent': 0, 'failure': 1, 'success': 2},\n",
    "    \"y\": {'no':0,'yes':1}\n",
    "}\n",
    "\n",
    "for col, mapping in label_encoding_dict.items():\n",
    "    if col in df_processed.columns:\n",
    "        df_processed[col] = df_processed[col].map(mapping)\n",
    "\n",
    "# === 4. One-Hot Encoding ===\n",
    "one_hot_features = [\"job\", \"marital\"]\n",
    "df_processed = pd.get_dummies(df_processed, columns=one_hot_features, dtype=int)\n",
    "\n",
    "# === 5. 处理数值特征 ===\n",
    "numeric_features = [\"age\", \"campaign\", \"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\"]\n",
    "\n",
    "# 缺失值填充（均值填充）适用于数值变量\n",
    "mean_imputer = SimpleImputer(strategy=\"mean\")\n",
    "df_processed[numeric_features] = mean_imputer.fit_transform(df_processed[numeric_features])\n",
    "\n",
    "# 标准化 (Standardization)\n",
    "scaler_standard = StandardScaler()\n",
    "df_processed[numeric_features] = scaler_standard.fit_transform(df_processed[numeric_features])\n",
    "\n",
    "# # 归一化 (Normalization)\n",
    "# scaler_minmax = MinMaxScaler()\n",
    "# df_processed[numeric_features] = scaler_minmax.fit_transform(df_processed[numeric_features])\n",
    "\n",
    "# === 6. K-Fold交叉验证 ===\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "save_dir = \"/Users/cleazhang/Downloads/bank-additional-dataset\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 进行 KFold 划分，并保存 train 和 test\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kf.split(df_processed)):\n",
    "    train_data = df_processed.iloc[train_idx]\n",
    "    test_data = df_processed.iloc[test_idx]\n",
    "\n",
    "    # === 分别保存 train 和 test 数据 ===\n",
    "    train_path = os.path.join(save_dir, f\"kfold_fold_{fold_idx+1}_train.csv\")\n",
    "    test_path = os.path.join(save_dir, f\"kfold_fold_{fold_idx+1}_test.csv\")\n",
    "\n",
    "    train_data.to_csv(train_path, index=False)  # 保存为 CSV\n",
    "    test_data.to_csv(test_path, index=False)\n",
    "# === 打印处理后的数据 ===\n",
    "#print(df_processed.isna().sum().sum())  # 计算整个 DataFrame 里的 NaN 总数\n",
    "#nan_columns = df_processed.columns[df_processed.isna().sum() > 0]  # 找出含 NaN 的列\n",
    "# print(\"Columns with NaN values:\\n\", df_processed[nan_columns].isna().sum())\n",
    "\n",
    "# print(\"NaN values:\\n\", df_processed.isna().sum())  # 检查 NaN\n",
    "# print(\"Infinite values:\\n\", (df_processed == np.inf).sum() + (df_processed == -np.inf).sum())  # 检查无穷大\n",
    "\n",
    "print(\"\\n最终处理后的数据：\")\n",
    "print(df_processed.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
