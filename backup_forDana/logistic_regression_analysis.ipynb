{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafe07be",
   "metadata": {},
   "source": [
    "# üîç Logistic Regression Classifier: Full Train vs Test Metrics with K-Fold CV and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544292d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_logreg_kfold():\n",
    "    metrics = {\n",
    "        'train': {'accuracy': [], 'precision': [], 'recall': [], 'f1_macro': [], 'f1_weighted': [], 'f1_class0': [], 'f1_class1': []},\n",
    "        'test': {'accuracy': [], 'precision': [], 'recall': [], 'f1_macro': [], 'f1_weighted': [], 'f1_class0': [], 'f1_class1': []}\n",
    "    }\n",
    "    global train_conf_matrices, test_conf_matrices\n",
    "    train_conf_matrices = []\n",
    "    test_conf_matrices = []\n",
    "    label_column = 'y'\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        train_path = f'kfold_fold_{i}_train.csv'\n",
    "        test_path = f'kfold_fold_{i}_test.csv'\n",
    "        df_train = pd.read_csv(train_path)\n",
    "        df_test = pd.read_csv(test_path)\n",
    "\n",
    "        X_train = df_train.drop(label_column, axis=1)\n",
    "        y_train = df_train[label_column]\n",
    "        X_test = df_test.drop(label_column, axis=1)\n",
    "        y_test = df_test[label_column]\n",
    "\n",
    "        model = LogisticRegression(solver='liblinear')\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        for dataset, y_true, y_pred in [('train', y_train, y_train_pred), ('test', y_test, y_test_pred)]:\n",
    "            metrics[dataset]['accuracy'].append(accuracy_score(y_true, y_pred))\n",
    "            metrics[dataset]['precision'].append(precision_score(y_true, y_pred, zero_division=0))\n",
    "            metrics[dataset]['recall'].append(recall_score(y_true, y_pred, zero_division=0))\n",
    "            metrics[dataset]['f1_macro'].append(f1_score(y_true, y_pred, average='macro', zero_division=0))\n",
    "            metrics[dataset]['f1_weighted'].append(f1_score(y_true, y_pred, average='weighted', zero_division=0))\n",
    "            metrics[dataset]['f1_class0'].append(f1_score(y_true, y_pred, pos_label=0, zero_division=0))\n",
    "            metrics[dataset]['f1_class1'].append(f1_score(y_true, y_pred, pos_label=1, zero_division=0))\n",
    "\n",
    "        train_conf_matrices.append(confusion_matrix(y_train, y_train_pred))\n",
    "        test_conf_matrices.append(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "        print(f\"Fold {i} - Train Accuracy: {metrics['train']['accuracy'][-1]:.4f}, Test Accuracy: {metrics['test']['accuracy'][-1]:.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25903829",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_logreg_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize all Train vs Test Metrics\n",
    "folds = list(range(1, 11))\n",
    "measures = ['accuracy', 'precision', 'recall', 'f1_macro', 'f1_weighted', 'f1_class0', 'f1_class1']\n",
    "\n",
    "for measure in measures:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(folds, metrics['train'][measure], marker='o', label=f'Train {measure}')\n",
    "    plt.plot(folds, metrics['test'][measure], marker='o', label=f'Test {measure}')\n",
    "    plt.title(f'Train vs Test {measure.replace('_', ' ').title()}')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel(measure.replace('_', ' ').title())\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7481899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Summary Table and CSV Export\n",
    "summary_data = {}\n",
    "for split in ['train', 'test']:\n",
    "    for metric, values in metrics[split].items():\n",
    "        summary_data[f'{split}_{metric}_mean'] = [np.mean(values)]\n",
    "        summary_data[f'{split}_{metric}_std'] = [np.std(values)]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).T\n",
    "summary_df.columns = ['Value']\n",
    "summary_df.to_excel('logreg_metrics_report.xlsx')\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55009238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Save Confusion Matrices\n",
    "os.makedirs('confusion_matrices_logreg/train', exist_ok=True)\n",
    "os.makedirs('confusion_matrices_logreg/test', exist_ok=True)\n",
    "\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(train_conf_matrices[i]).plot(ax=ax)\n",
    "    plt.title(f'Train Confusion Matrix - Fold {i+1}')\n",
    "    plt.savefig(f'confusion_matrices_logreg/train/fold_{i+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ConfusionMatrixDisplay(test_conf_matrices[i]).plot(ax=ax)\n",
    "    plt.title(f'Test Confusion Matrix - Fold {i+1}')\n",
    "    plt.savefig(f'confusion_matrices_logreg/test/fold_{i+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "print('Saved all confusion matrices.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
