{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 正在處理 Fold 1 ===\n",
      "\n",
      "=== 正在處理 Fold 2 ===\n",
      "\n",
      "=== 正在處理 Fold 3 ===\n",
      "\n",
      "=== 正在處理 Fold 4 ===\n",
      "\n",
      "=== 正在處理 Fold 5 ===\n",
      "\n",
      "=== 正在處理 Fold 6 ===\n",
      "\n",
      "=== 正在處理 Fold 7 ===\n",
      "\n",
      "=== 正在處理 Fold 8 ===\n",
      "\n",
      "=== 正在處理 Fold 9 ===\n",
      "\n",
      "=== 正在處理 Fold 10 ===\n",
      "\n",
      "=== 10 折交叉驗證最終結論 ===\n",
      "              Accuracy        Precision           Recall       Class 0 F1  \\\n",
      "Train  0.9995 ± 0.0002  0.9981 ± 0.0006  0.9992 ± 0.0012  0.9997 ± 0.0001   \n",
      "Test   0.8946 ± 0.0126  0.7308 ± 0.0465  0.5901 ± 0.0187  0.9430 ± 0.0072   \n",
      "\n",
      "            Class 1 F1         F1 Score Weighted F1 Score              AUC  \n",
      "Train  0.9975 ± 0.0011  0.9986 ± 0.0006   0.9995 ± 0.0002  0.9999 ± 0.0000  \n",
      "Test   0.2920 ± 0.0471  0.6175 ± 0.0251   0.8719 ± 0.0166  0.7492 ± 0.0333  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # ✅ 引入随机森林\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "train_confusion_matrices_sum = np.zeros((2, 2), dtype=int)\n",
    "test_confusion_matrices_sum = np.zeros((2, 2), dtype=int)\n",
    "\n",
    "metrics_dict = {\n",
    "    'train_accuracy': [], 'test_accuracy': [],\n",
    "    'train_precision': [], 'test_precision': [],\n",
    "    'train_recall': [], 'test_recall': [],\n",
    "    'train_class0_f1': [], 'test_class0_f1': [],\n",
    "    'train_class1_f1': [], 'test_class1_f1': [],\n",
    "    'train_f1': [], 'test_f1': [],\n",
    "    'train_weighted_f1': [], 'test_weighted_f1': [],\n",
    "    'train_auc': [], 'test_auc': []\n",
    "}\n",
    "\n",
    "for fold in range(1, num_folds + 1):\n",
    "    print(f\"\\n=== 正在處理 Fold {fold} ===\")\n",
    "\n",
    "    train_file = f'/Users/cleazhang/Downloads/bank-additional-dataset/kfold_fold_{fold}_train.csv'\n",
    "    test_file = f'/Users/cleazhang/Downloads/bank-additional-dataset/kfold_fold_{fold}_test.csv'\n",
    "\n",
    "    df_train = pd.read_csv(train_file)\n",
    "    df_test = pd.read_csv(test_file)\n",
    "    df_train.drop(columns=['Unnamed: 0'], errors='ignore', inplace=True)\n",
    "    df_test.drop(columns=['Unnamed: 0'], errors='ignore', inplace=True)\n",
    "\n",
    "    df_train_class = df_train['y']\n",
    "    df_train_features = df_train.drop(columns='y')\n",
    "    df_test_class = df_test['y']\n",
    "    df_test_features = df_test.drop(columns='y')\n",
    "\n",
    "    ### ✅ 使用随机森林\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf_model.fit(df_train_features, df_train_class)\n",
    "\n",
    "    predicted_train = rf_model.predict(df_train_features)\n",
    "    predicted_test = rf_model.predict(df_test_features)\n",
    "    predicted_prob_train = rf_model.predict_proba(df_train_features)\n",
    "    predicted_prob_test = rf_model.predict_proba(df_test_features)\n",
    "\n",
    "    train_confusion_matrices_sum += confusion_matrix(df_train_class, predicted_train)\n",
    "    test_confusion_matrices_sum += confusion_matrix(df_test_class, predicted_test)\n",
    "\n",
    "    # 指标计算\n",
    "    train_accuracy = rf_model.score(df_train_features, df_train_class)\n",
    "    test_accuracy = rf_model.score(df_test_features, df_test_class)\n",
    "    metrics_dict['train_accuracy'].append(train_accuracy)\n",
    "    metrics_dict['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "    train_precision = precision_score(df_train_class, predicted_train, average='macro')\n",
    "    test_precision = precision_score(df_test_class, predicted_test, average='macro')\n",
    "    train_recall = recall_score(df_train_class, predicted_train, average='macro')\n",
    "    test_recall = recall_score(df_test_class, predicted_test, average='macro')\n",
    "\n",
    "    train_f1_per_class = f1_score(df_train_class, predicted_train, average=None)\n",
    "    test_f1_per_class = f1_score(df_test_class, predicted_test, average=None)\n",
    "    train_f1 = f1_score(df_train_class, predicted_train, average='macro')\n",
    "    test_f1 = f1_score(df_test_class, predicted_test, average='macro')\n",
    "    train_weighted_f1 = f1_score(df_train_class, predicted_train, average='weighted')\n",
    "    test_weighted_f1 = f1_score(df_test_class, predicted_test, average='weighted')\n",
    "\n",
    "    metrics_dict['train_precision'].append(train_precision)\n",
    "    metrics_dict['test_precision'].append(test_precision)\n",
    "    metrics_dict['train_recall'].append(train_recall)\n",
    "    metrics_dict['test_recall'].append(test_recall)\n",
    "    metrics_dict['train_class0_f1'].append(train_f1_per_class[0])\n",
    "    metrics_dict['test_class0_f1'].append(test_f1_per_class[0])\n",
    "    metrics_dict['train_class1_f1'].append(train_f1_per_class[1])\n",
    "    metrics_dict['test_class1_f1'].append(test_f1_per_class[1])\n",
    "    metrics_dict['train_f1'].append(train_f1)\n",
    "    metrics_dict['test_f1'].append(test_f1)\n",
    "    metrics_dict['train_weighted_f1'].append(train_weighted_f1)\n",
    "    metrics_dict['test_weighted_f1'].append(test_weighted_f1)\n",
    "\n",
    "    train_auc = roc_auc_score(df_train_class, predicted_prob_train[:, 1])\n",
    "    test_auc = roc_auc_score(df_test_class, predicted_prob_test[:, 1])\n",
    "    metrics_dict['train_auc'].append(train_auc)\n",
    "    metrics_dict['test_auc'].append(test_auc)\n",
    "\n",
    "# === 输出平均性能指标 ===\n",
    "metrics_summary = {\n",
    "    metric: f\"{np.mean(values):.4f} ± {np.std(values):.4f}\" for metric, values in metrics_dict.items()\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Accuracy': [metrics_summary['train_accuracy'], metrics_summary['test_accuracy']],\n",
    "    'Precision': [metrics_summary['train_precision'], metrics_summary['test_precision']],\n",
    "    'Recall': [metrics_summary['train_recall'], metrics_summary['test_recall']],\n",
    "    'Class 0 F1': [metrics_summary['train_class0_f1'], metrics_summary['test_class0_f1']],\n",
    "    'Class 1 F1': [metrics_summary['train_class1_f1'], metrics_summary['test_class1_f1']],\n",
    "    'F1 Score': [metrics_summary['train_f1'], metrics_summary['test_f1']],\n",
    "    'Weighted F1 Score': [metrics_summary['train_weighted_f1'], metrics_summary['test_weighted_f1']],\n",
    "    'AUC': [metrics_summary['train_auc'], metrics_summary['test_auc']]\n",
    "}, index=['Train', 'Test'])\n",
    "\n",
    "print(\"\\n=== 10 折交叉驗證最終結論 ===\")\n",
    "print(summary_df)\n",
    "\n",
    "summary_df.to_csv('model_evaluation_summary_rf.csv')\n",
    "np.savetxt('train_confusion_matrix_sum_rf.csv', train_confusion_matrices_sum, delimiter=',', fmt='%d')\n",
    "np.savetxt('test_confusion_matrix_sum_rf.csv', test_confusion_matrices_sum, delimiter=',', fmt='%d')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
